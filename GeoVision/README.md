# GeoVision

> 2019년, 학부 연구실([InfoLab](http://infolab.kunsan.ac.kr/), 구 MCALab)에서 진행한 연구 프로젝트 **Urban AI Network** 솔루션 중 내가 맡은 SubProject. 해당 대회 시연 당시에는 베타였으며, 이후 교내 대회 출품 시점에서 1.0 완성.
[관련기사1](https://www.ebn.co.kr/news/view/1014791?kind=&key=&shword=&page=4144&period=)
[관련기사2](https://wowtale.net/2019/12/24/socar-and-kiise-hosted-ai-video-analysis-contest/)

<p align="center">
  <img src="https://github.com/escaco95/ARCODA-Choismind/blob/main/GeoVision/DeepGeoEvaluator.png?raw=true" width="400" height="250"><img src="https://github.com/escaco95/ARCODA-Choismind/blob/main/GeoVision/GeoVision Task Manager.png?raw=true" width="400" height="250">
</p>

`좌측:일괄 처리 매니저의 중간 과정 점검용 유틸리티`
`우측:솔루션 전 과정에 대한 일괄 처리 매니저`

**팀 구성**

 1. 팀장 : 머신 러닝 모델 구현
 1. 팀원 : 웹 뷰어 및 API 서버 구현
 1. 본인 : 시스템 파이프라인 구축

**솔루션의 기본적인 틀:**
 - 블랙박스로부터 시내 주행 영상 + 좌표 데이터 수집
 - 다양한 차량, 다양한 블랙박스에서 수집된 영상 분석
> 어떤 도시 개체(신호등,차량,소화전 등)들이 등장했는지 검출합니다.
 - 분석된 정보를 정합하여 검출된 개체의 이동 경로(추적된) 및 공간 좌표를 시스템에 수집

**솔루션의 궁극적 목표:**
 - 외부 사용자로부터 전달된 데이터도 수집하게 하면, 데이터셋 허브의 역할도 기대할 수 있을 것
 - 단순한 공간 데이터가 아닌 **시공간** 정보를 기록함으로써 도시와 관련된 다양한 의사결정의 바탕에 시간의 흐름에 따른 변화 역시 반영할 수 있게 될 것 

**먼 미래, 완성된 데이터를 바탕으로 한 차후 용례:**
 - 교통사고 발생 이후 가해자 도주
 - 시스템 질의 : UTC 18시 30분 경에 XX사거리 인근 50m 내에 붉은색 승용차 검색
 - 검색조건에 해당하는 차량들과 추적된 경로가 지도에 표시, 경로를 클릭하면 해당 차량이 촬영된 시점의 영상들을 정확한 시점으로 확인할 수 있음
 - PROFIT?

**최종 데이터:**

<p align="center">
  <img src="https://github.com/escaco95/ARCODA-Choismind/blob/main/GeoVision/GeoVisionResultSample.gif?raw=true" width="400" height="250">
</p>

> 여러 프레임에 걸쳐 등장한 차랑을 **동일 차량(37번)** 으로 잘 추적해낸 사례. 촬영자 GPS 정보와 화면 상 등장 좌표 및 각도를 바탕으로 계산된 상대적 경로 역시 오른쪽으로 잘 추적되어 있다. 다만 위성 지도를 사용하느냐, 벡터 지도를 사용하느냐에 따라 구글 지도에 오차가 있어서 차선 절반이 어긋난 것 처럼 보이는 건 함정.

**프로젝트 준비:**

 - 개체 검출 모델은 팀장([DongJun Ryu](http://mcalab.kunsan.ac.kr/?mid=board_qvke37&document_srl=238))으로부터 넘겨받으므로 그 부분은 패스
> 구축된 시스템 Pipeline 안에 개체 검출 모델의 Input Output이 딱 맞도록 조정해주는 작업이 필요했다.
- 개체가 잘 검출되었는지 육안으로 확인하는 과정이 필요
> 중간 점검 유틸리티를 구현. 이미지 파일과 json 마스크를 메모리에 미리 불러와놓고 슬라이드 바를 주르륵 드래그하여 확인하는 과정을 통해 파일 탐색기 썸네일을 들여다보는 시간을 절약할 수 있었다.
 - 여러 프레임에서 검출된 개체는 전부 다른 라벨링이 부착됨
> 분명 같은 자동차인데 직전 프레임에서는 car36, 다음 프레임에서는 car54. 개체를 추적해서 라벨을 타임라인으로 merge 하는 과정이 요구되었다. 이는 나중에 파이프라인 모듈의 일부로 구현했다.

**시스템 파이프라인**

어떤 영상 소스들이 인풋으로 들어오고, 어떤 처리들이 필요한지를 팀 내에서 논의해본 결과 아래의 **8 + 1** 개의 과정으로 구성하기로 했다.
비디오의 코덱도 다양하고, 전처리도 다양하게 적용할 필요성이 있고, mask-rcnn, yolov3, fast-rcnn등의 다양한 모델을 적용해보고 싶을 수도 있으니 사실상 전 과정은 모듈화될 필요성이 있었다.

 1. 비디오 분해
 1. 영상 전처리 (Crop 한다거나, 해상도를 낮춘다거나 등)
 1. 개체 검출기 (팀장으로부터 넘겨받은 모듈 사용)
 1. 필터 (tree만 검출하거나, transportation에 대해서만 검출하고 싶을 수도 있으니까. 또는 뜬금없이 등장하는 boat를 걸러내는 역할도 수행한다)
 1. GPS 추출 (영상마다 GPS 정보를 담고 있는 포맷이 다르다. 추출하는 방식도 제각각. 블랙박스 업체의 기술지원을 받아 모듈을 완성하였다)
 1. 개체 위치 추정기 (GPS 보간. 영상에 등장한 개체의 bbox 또는 폴리곤 크기, 화면 상의 좌표 차이 및 삼각측량을 이용하여 상대 좌표를 추정한다)
 1. 개체 추적기 (위 과정을 거쳐 대략적인 시공간 및 화상 데이터를 얻어냈기 때문에, Template Matching 및 좌표 비교를 통해 가장 유사한 개체와 Merge 시킨다)
 1. 후처리 (API 서버에 넘겨주기 전에 메타데이터를 부착하는 작업 등을 수행한다)

